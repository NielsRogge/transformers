.. 
    Copyright 2020 The HuggingFace Team. All rights reserved.

    Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
    the License. You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
    an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
    specific language governing permissions and limitations under the License.

LUKE
-----------------------------------------------------------------------------------------------------------------------

Overview
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The LUKE model was proposed in `LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention
<https://arxiv.org/abs/2010.01057>`_ by Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda and Yuji Matsumoto. 
It is based on RoBERTa and adds entity embeddings as well as an entity-aware self-attention mechanism, which helps improve 
performance on several downstream tasks involving reasoning about entities such as named-entity recognition, extractive and 
cloze-style question answering, entity linking and relation classification between entities. 

The abstract from the paper is the following:

*Entity representations are useful in natural language tasks involving entities. In this paper, we propose new pretrained 
contextualized representations of words and entities based on the bidirectional transformer. The proposed model treats words 
and entities in a given text as independent tokens, and outputs contextualized representations of them. Our model is trained 
using a new pretraining task based on the masked language model of BERT. The task involves predicting randomly masked words 
and entities in a large entity-annotated corpus retrieved from Wikipedia. We also propose an entity-aware self-attention mechanism 
that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) 
when computing attention scores. The proposed model achieves impressive empirical performance on a wide range of entity-related 
tasks. In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED 
(relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 
(extractive question answering).*

Tips:

- This implementation is the same as :class:`~transformers.RobertaModel` with the addition of entity embeddings as well as an entity-
  aware self-attention mechanism, which improves performance on tasks involing reasoning about entities.
- LUKE adds :obj:`entity_ids`, :obj:`entity_attention_mask`, :obj:`entity_token_type_ids` and :obj:`entity_position_ids` as extra input
  to the model. You can obtain those using :class:`LukeTokenizer`.

The original code can be found `here <https://github.com/studio-ousia/luke>`_.


LukeConfig
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. autoclass:: transformers.LukeConfig
    :members:


LukeModel
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. autoclass:: transformers.LukeModel
    :members: forward


LukeForMaskedLM
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. autoclass:: transformers.LukeForMaskedLM
    :members: forward


LukeEntityAwareAttentionModel
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. autoclass:: transformers.LukeEntityAwareAttentionModel
    :members: forward
