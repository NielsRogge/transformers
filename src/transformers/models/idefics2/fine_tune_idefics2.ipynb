{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune Idefics2 for document parsing (PDF -> JSON)\n",
    "\n",
    "In this notebook, we are going to fine-tune the [Idefics2](https://huggingface.co/docs/transformers/main/en/model_doc/idefics2) model for a document AI use case. Idefics2 is one of the best open-source multimodal models at the time of writing, developed by Hugging Face. Idefics started as a replication of Deepmind's Flamingo model, and the second iteration incorporates a lot of advancements in the field such as [NaViT](https://arxiv.org/abs/2307.06304) patching, [Perceiver](https://arxiv.org/abs/2103.03206) resampling and more. However, explaining how Idefics2 works would desire its own video which I might put on YouTube!\n",
    "\n",
    "The goal for the model is to generate a JSON that contains key fields (like food items and their corresponding prices) from receipts. We will fine-tune Idefics2 on the [CORD](https://huggingface.co/datasets/naver-clova-ix/cord-v2) dataset, which contains (receipt image, ground truth JSON) pairs.\n",
    "\n",
    "Sources:\n",
    "\n",
    "* Idefics2 [blog post](https://huggingface.co/blog/idefics2)\n",
    "* Idefics2 [DocVQA notebook](https://colab.research.google.com/drive/1NtcTgRbSBKN7pYD3Vdx1j9m8pt3fhFDB?usp=sharing#scrollTo=06CMDrH7Kkdy) on which this notebook is based \n",
    "\n",
    "Note: this notebook is a direct adaptation of my [original Donut notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Donut/CORD/Fine_tune_Donut_on_a_custom_dataset_(CORD)_with_PyTorch_Lightning.ipynb) for Idefics2. You can view Idefics2 as a more powerful Donut model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "Let's start by loading the dataset from the hub. Here we use the [CORD](https://huggingface.co/datasets/naver-clova-ix/cord-v2) dataset, created by the [Donut](https://huggingface.co/docs/transformers/en/model_doc/donut) authors (Donut is another powerful - but slightly undertrained document AI model available in the Transformers library). CORD is an important benchmark for receipt understanding. The Donut authors have prepared it in a format that suits vision-language models: we're going to fine-tune it to generate the JSON given the image.\n",
    "\n",
    "If you want to load your own custom dataset, check out this guide: https://huggingface.co/docs/datasets/image_dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"naver-clova-ix/cord-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As oftentimes, we get a `DatasetDict` which is a dictionary containing 3 splits, one for training, validation and testing. Each split has 2 features, an image and a corresponding ground truth.\n",
    "\n",
    " Let's check the first training example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset['train'][0]\n",
    "image = example[\"image\"]\n",
    "# resize image for smaller displaying\n",
    "width, height = image.size\n",
    "image = image.resize((int(0.3*width), int(0.3*height)))\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the corresponding ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example[\"ground_truth\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! So this contains a ground truth parsing that we want the model to output given an image. We can read it as json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json.loads(example[\"ground_truth\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load processor\n",
    "\n",
    "Next, we'll load the processor which is used to prepare the data in the format that the model expects. Neural networks like Idefics2 don't directly take images and text as input, but rather `pixel_values` (which is a resized, rescaled, normalized and optionally splitted version of the receipt images), `input_ids` (which are text token indices in the vocabulary of the model), etc. This is handled by the processor.\n",
    "\n",
    "### Image splitting\n",
    "\n",
    "Idefics2's processor has a setting called `do_image_splitting` which can be set to `True`/`False`. This defines how images are prepared for the model, either it will just create 1 image (with `do_image_splitting=False`) or it will create multiple (by splitting the images into multiple patches and also including the original image).\n",
    "\n",
    "This has an effect on the amount of memory that's going to be used during training: if we use image splitting we'll encounter more memory usage (as several images are created for each receipt image). We'll use the memory friendly version here. Do note that this has an effect on performance; performance is typically higher with the `do_image_splitting=True` setting. The latter is also how the model which we're going to use (https://huggingface.co/HuggingFaceM4/idefics2-8b) was trained.\n",
    "\n",
    "See also from the [model card](https://huggingface.co/HuggingFaceM4/idefics2-8b):\n",
    "\n",
    "> do_image_splitting=True is especially needed to boost performance on OCR tasks where a very large image is used as input. For the regular VQA or captioning tasks, this argument can be safely set to False with minimal impact on performance (see the evaluation table above).\n",
    "\n",
    "### Image resolution\n",
    "\n",
    "Additionally, one can decrease the maximum image resolution used during training to decrease memory usage. To do so, add `size= {\"longest_edge\": 448, \"shortest_edge\": 378}` when initializing the processor (`AutoProcessor.from_pretrained`). In particular, the longest_edge value can be adapted to fit the need (the default value is 980). We recommend using values that are multiples of 14. There are no changes required on the model side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"HuggingFaceM4/idefics2-8b\", do_image_splitting=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model\n",
    "\n",
    "Next, we're going to load the Idefics2 model from the [hub](https://huggingface.co/HuggingFaceM4/idefics2-8b). This is a model with 8 billion trainable parameters. Do note that we load a model here which already has undergone supervised fine-tuning (SFT). The pure pre-trained model (also called \"base model\") is available here: https://huggingface.co/HuggingFaceM4/idefics2-8b-base. We can benefit from the fine-tuning that the model already has undergone.\n",
    "\n",
    "Do note that the Idefics2 team is also going to release a chatty version of Idefics2 optimized for chatbot/AI assistant use cases. However in our case we do not care about the chatty aspect, we just want the model to generate perfect JSON given an image of a receipt.\n",
    "\n",
    "### Full fine-tuning, LoRa and Q-LoRa\n",
    "\n",
    "As this model has 8 billion trainable parameters, that's going to have quite an impact on the amount of memory used. For reference, fine-tuning a model using the [AdamW optimizer](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW) (which is often used to optimize neural networks) with mixed precision, you need about 18 times the amount of parameters in GB of GPU RAM. So in this case, we would need 18x8 billion bytes = 144 GB of GPU RAM if we want to update all the parameters of the model!! That's huge right? And for most people infeasible.\n",
    "\n",
    "Luckily, some clever people came up with the [LoRa](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora) method (LoRa is short for low-rank adapation). It allows to just freeze the existing weights and only train a couple of adapter layers on top of the base model. Hugging Face offers the separate [PEFT library](https://huggingface.co/docs/peft/main/en/index) for easy use of LoRa, along with other Parameter-Efficient Fine-Tuning methods (that's where the name PEFT comes from).\n",
    "\n",
    "Moreover, one can not only freeze the existing base model but also quantize it (which means, shrinking down its size). A neural network's parameters are typically saved in either float32 (which means, 32 bits or 4 bytes are used to store each parameter value) or float16 (which means, 16 bits or half a byte - also called half precision). However, with some clever algorithms one can shrink each parameter to just 8 or 4 bits (half a byte!), without significant effect on final performance. Read all about it here: https://huggingface.co/blog/4bit-transformers-bitsandbytes.\n",
    "\n",
    "This means that we're going to shrink the size of the base Idefics2-8b model considerably using 4-bit quantization, and then only train a couple of adapter layers on top using LoRa (in float16). This idea of combining LoRa with quantization is called Q-LoRa and is the most memory friendly version.\n",
    "\n",
    "Of course, if you have the memory available, feel free to use full fine-tuning or LoRa without quantization! In case of full fine-tuning, the code snippet below instantiates the model with Flash Attention which considerably speeds up computations.\n",
    "\n",
    "There exist many forms of quantization, here we leverage the [BitsAndBytes](https://huggingface.co/docs/transformers/main_classes/quantization#transformers.BitsAndBytesConfig) integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig, Idefics2ForConditionalGeneration\n",
    "from peft import LoraConfig\n",
    "import torch\n",
    "\n",
    "\n",
    "DEVICE = \"cuda:0\"\n",
    "USE_LORA = False\n",
    "USE_QLORA = True\n",
    "\n",
    "## Load model\n",
    "\n",
    "# Three options for training, from the lowest precision training to the highest precision training:\n",
    "# - QLora\n",
    "# - Standard Lora\n",
    "# - Full fine-tuning\n",
    "if USE_QLORA or USE_LORA:\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=8,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=\".*(text_model|modality_projection|perceiver_resampler).*(down_proj|gate_proj|up_proj|k_proj|q_proj|v_proj|o_proj).*$\",\n",
    "        use_dora=False if USE_QLORA else True,\n",
    "        init_lora_weights=\"gaussian\",\n",
    "    )\n",
    "    if USE_QLORA:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "    model = Idefics2ForConditionalGeneration.from_pretrained(\n",
    "        \"HuggingFaceM4/idefics2-8b\",\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config=bnb_config if USE_QLORA else None,\n",
    "    )\n",
    "    model.add_adapter(lora_config)\n",
    "    model.enable_adapters()\n",
    "else:\n",
    "    model = Idefics2ForConditionalGeneration.from_pretrained(\n",
    "        \"HuggingFaceM4/idefics2-8b\",\n",
    "        torch_dtype=torch.float16,\n",
    "        _attn_implementation=\"flash_attention_2\",  # Only available on A100 or H100\n",
    "    ).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PyTorch dataset\n",
    "\n",
    "Next we'll create a regular [PyTorch dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html). For that, one needs to implement 3 methods: an `init` method, a `len` method (which returns the length of the dataset) and a `getitem` method (which returns items of the dataset).\n",
    "\n",
    "The `init` method implements 2 things:\n",
    "* it goes over all the ground truth JSON sequences and turns them into token sequences (which we want the model to generate) using the `json2token` method\n",
    "* it adds special tokens to the model/tokenizer using the `add_tokens` method for which the model will learn an embedding vector. By doing this, keys which occur in the JSON sequences (like `<menu>` in our case) will get their own token (and corresponding embedding), whereas otherwise these might have been split up into multiple tokens. Do note that I haven't quantified the performance difference regarding this, not sure it helps a lot but the Donut authors did this so I assume it must benefit training.\n",
    "\n",
    "Typically, one uses the processor in the `getitem` method to prepare the data in the format that the model expects, but we'll postpone that here for a reason we'll explain later. In our case we're just going to return 2 things: the image and a corresponding ground truth token sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Any, List\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "added_tokens = []\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hf_dataset,\n",
    "        split,\n",
    "        sort_json_key: bool = True,\n",
    "    ):\n",
    "        self.dataset = hf_dataset[split]\n",
    "        self.split = split\n",
    "        self.sort_json_key = sort_json_key\n",
    "\n",
    "        ground_truth_token_sequences = []\n",
    "        for sample in self.dataset:\n",
    "            ground_truth = json.loads(sample[\"ground_truth\"])\n",
    "            if \"gt_parses\" in ground_truth:  # some datasets have multiple ground truths available, e.g. DocVQA\n",
    "                assert isinstance(ground_truth[\"gt_parses\"], list)\n",
    "                ground_truth_jsons = ground_truth[\"gt_parses\"]\n",
    "            else:\n",
    "                assert \"gt_parse\" in ground_truth and isinstance(ground_truth[\"gt_parse\"], dict)\n",
    "                ground_truth_jsons = [ground_truth[\"gt_parse\"]]\n",
    "\n",
    "            ground_truth_token_sequences.append(\n",
    "                [\n",
    "                    self.json2token(\n",
    "                        ground_truth_json,\n",
    "                        update_special_tokens_for_json_key=self.split == \"train\",\n",
    "                        sort_json_key=self.sort_json_key,\n",
    "                    )\n",
    "                    for ground_truth_json in ground_truth_jsons  # load json from list of json\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        self.ground_truth_token_sequences = ground_truth_token_sequences\n",
    "\n",
    "    def json2token(self, obj: Any, update_special_tokens_for_json_key: bool = True, sort_json_key: bool = True):\n",
    "        \"\"\"\n",
    "        Convert an ordered JSON object into a token sequence\n",
    "        \"\"\"\n",
    "        if type(obj) == dict:\n",
    "            if len(obj) == 1 and \"text_sequence\" in obj:\n",
    "                return obj[\"text_sequence\"]\n",
    "            else:\n",
    "                output = \"\"\n",
    "                if sort_json_key:\n",
    "                    keys = sorted(obj.keys(), reverse=True)\n",
    "                else:\n",
    "                    keys = obj.keys()\n",
    "                for k in keys:\n",
    "                    if update_special_tokens_for_json_key:\n",
    "                        self.add_tokens([rf\"<s_{k}>\", rf\"</s_{k}>\"])\n",
    "                    output += (\n",
    "                        rf\"<s_{k}>\"\n",
    "                        + self.json2token(obj[k], update_special_tokens_for_json_key, sort_json_key)\n",
    "                        + rf\"</s_{k}>\"\n",
    "                    )\n",
    "                return output\n",
    "        elif type(obj) == list:\n",
    "            return r\"<sep/>\".join(\n",
    "                [self.json2token(item, update_special_tokens_for_json_key, sort_json_key) for item in obj]\n",
    "            )\n",
    "        else:\n",
    "            obj = str(obj)\n",
    "            if f\"<{obj}/>\" in added_tokens:\n",
    "                obj = f\"<{obj}/>\"  # for categorical special tokens\n",
    "            return obj\n",
    "\n",
    "    def add_tokens(self, list_of_tokens: List[str]):\n",
    "        \"\"\"\n",
    "        Add special tokens to tokenizer and resize the token embeddings of the decoder\n",
    "        \"\"\"\n",
    "        newly_added_num = processor.tokenizer.add_tokens(list_of_tokens)\n",
    "        if newly_added_num > 0:\n",
    "            model.resize_token_embeddings(len(processor.tokenizer))\n",
    "            added_tokens.extend(list_of_tokens)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.dataset[idx]\n",
    "        # get the image and corresponding target token sequence\n",
    "        image = example[\"image\"]\n",
    "        target_sequence = random.choice(self.ground_truth_token_sequences[idx])  # can be more than one, e.g., DocVQA\n",
    "\n",
    "        return image, target_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate the PyTorch datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(hf_dataset=dataset, split=\"train\")\n",
    "eval_dataset = CustomDataset(hf_dataset=dataset, split=\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check one item of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check how many tokens were added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Added {len(added_tokens)} tokens to the model/tokenizer\")\n",
    "print(added_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify that special tokens are now known by the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.tokenizer.tokenize(\"</s_total_price>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define DataCollator\n",
    "\n",
    "Now that we have a PyTorch dataset, we'll define a so-called collator which defines how items of the dataset should be batched together. This is because we tyipcally train neural networks on batches of data (i.e. various images/target sequences combined) rather than one-by-one, using a variant of stochastic-gradient descent or SGD (like Adam, AdamW, etc.).\n",
    "\n",
    "It's only here that we're going to use the processor to turn the (image, target token sequence) into the format that the model expects (which is `pixel_values`, `input_ids` etc.). The reason we do that here is because it allows for **dynamic padding** of the batches: each batch contains images of various resolutions (as Idefics2 preserves the aspect ratio of images). By only using the processor here, we will pad the pixel values up to the largest one in each batch (rather than padding them all to a fixed resolution upfront).\n",
    "\n",
    "Important here is that we are calling `apply_chat_template`, which applies a so-called chat template which turns the inputs into the format that the model expects. This is VERY important as this is how the model expects inputs to be formatted!! Read all about it here: https://huggingface.co/docs/transformers/main/en/chat_templating. We'll use the text prompt \"Extract JSON.\" which is also going to be used at inference time.\n",
    "\n",
    "We also decide to limit the length of the text tokens (`input_ids`) to 200 due to memory constraints, feel free to expand if your target token sequences are longer (I'd recommend plotting the average token length to determine the optimal value).\n",
    "\n",
    "Labels are created for the model by simply copying the inputs to the LLM (`input_ids`), but with padding tokens replaced by the ignore index of the loss function. This ensures that the model doesn't need to learn to predict padding tokens (used to batch examples together). For Idefics2, this is the `image_token_id` as can be seen [here](https://github.com/huggingface/transformers/blob/6f465d45d98f9eaeef83cfdfe79aecc7193b0f1f/src/transformers/models/idefics2/modeling_idefics2.py#L1860).\n",
    "\n",
    "Why are the labels a copy of the model inputs, you may ask? The model will internally shift the labels one position to the right so that the model will learn to predict the next token. This can be seen [here](https://github.com/huggingface/transformers/blob/6f465d45d98f9eaeef83cfdfe79aecc7193b0f1f/src/transformers/models/idefics2/modeling_idefics2.py#L1851-L1855)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataCollator:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "        self.image_token_id = processor.tokenizer.additional_special_tokens_ids[\n",
    "            processor.tokenizer.additional_special_tokens.index(\"<image>\")\n",
    "        ]\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        texts = []\n",
    "        images = []\n",
    "        for example in examples:\n",
    "            image, ground_truth = example\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": \"Extract JSON.\"},\n",
    "                        {\"type\": \"image\"},\n",
    "                    ],\n",
    "                },\n",
    "                {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": ground_truth}]},\n",
    "            ]\n",
    "            text = processor.apply_chat_template(messages, add_generation_prompt=False)\n",
    "            texts.append(text.strip())\n",
    "            images.append([image])\n",
    "\n",
    "        batch = processor(\n",
    "            text=texts, images=images, return_tensors=\"pt\", padding=True,\n",
    "        )\n",
    "\n",
    "        batch = processor(text=texts, images=images, padding=\"max_length\", max_length=200, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "        labels[labels == processor.tokenizer.pad_token_id] = self.image_token_id\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "    \n",
    "data_collator = MyDataCollator(processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define evaluation metrics\n",
    "\n",
    "During training, we'd like to compute some metrics on our evaluation set. For that we'll implement a `compute_metrics` function which takes in a list of predicted token sequences and a list of target token sequences, and we'll calculate the so-called Levenhstein edit distance. This quantifies how much we would need to edit the predicted token sequence to get the target sequence (the fewer edits the better!). Its optimal value is 0 (which means, no edits need to be made)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def normalized_levenshtein(s1, s2):\n",
    "    len_s1, len_s2 = len(s1), len(s2)\n",
    "    distance = Levenshtein.distance(s1, s2)\n",
    "    return distance / max(len_s1, len_s2)\n",
    "\n",
    "\n",
    "def similarity_score(a_ij, o_q_i, tau=0.5):\n",
    "    nl = normalized_levenshtein(a_ij, o_q_i)\n",
    "    return 1 - nl if nl < tau else 0\n",
    "\n",
    "\n",
    "def average_normalized_levenshtein_similarity(ground_truth, predicted_answers):\n",
    "    assert len(ground_truth) == len(predicted_answers), \"Length of ground_truth and predicted_answers must match.\"\n",
    "\n",
    "    N = len(ground_truth)\n",
    "    total_score = 0\n",
    "\n",
    "    for i in range(N):\n",
    "        a_i = ground_truth[i]\n",
    "        o_q_i = predicted_answers[i]\n",
    "        if o_q_i == \"\":\n",
    "            print(\"Warning: Skipped an empty prediction.\")\n",
    "            max_score = 0\n",
    "        else:\n",
    "            max_score = max(similarity_score(a_ij, o_q_i) for a_ij in a_i)\n",
    "\n",
    "        total_score += max_score\n",
    "\n",
    "    return total_score / N\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    # Replace -100s used for padding as we can't decode them\n",
    "    preds = np.where(preds != -100, preds, processor.tokenizer.pad_token_id)\n",
    "    decoded_preds = processor.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n",
    "    decoded_labels = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    print(\"Decoded predictions:\", decoded_preds)\n",
    "    print(\"Decoded labels:\", decoded_labels)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    score = average_normalized_levenshtein_similarity(decoded_labels, decoded_preds)\n",
    "    result = {\"levenshtein\": score}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != processor.tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training arguments\n",
    "\n",
    "There are various options to train a PyTorch model: one could just write a training loop themselves, use the [Trainer API](https://huggingface.co/docs/transformers/en/main_classes/trainer), use frameworks like PyTorch Lightning, etc.\n",
    "\n",
    "In this notebook, we'll use the `Seq2SeqTrainer` class, which is optimized for seq2seq models like T5 or BART (but will also work well in our case as we'll see). This class requires us to define `Seq2SeqTrainingArguments`. These define all hyperparameters regarding training. There are a lot more than the ones we define here, but most importantly we pass the batch size for training/evaluation, the number of warmup steps, the learning rate, how frequently we want to save the model, whether we want to use Weights and Biases logging, etc.\n",
    "\n",
    "Do note that I have not performed any hyperparameter optimization whatsoever so this could definitely be improved.\n",
    "\n",
    "See the full list here: https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, GenerationConfig\n",
    "\n",
    "generation_config = GenerationConfig.from_pretrained(\"HuggingFaceM4/idefics2-8b\", max_new_tokens=200)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    warmup_steps=50,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=25,\n",
    "    output_dir=\"idefics2_ft_tutorial\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=250,\n",
    "    save_total_limit=1,\n",
    "    fp16=True,\n",
    "    # push_to_hub_model_id=\"idefics2-8b-docvqa-finetuned-tutorial\",\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\",\n",
    "    predict_with_generate=True,\n",
    "    generation_config=generation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "The reason we use the `Seq2SeqTrainer` class is because it supports a `predict_with_generate` option, which allows us to use the [generate](https://huggingface.co/docs/transformers/v4.40.1/en/main_classes/text_generation#transformers.GenerationMixin.generate) method (typically used at inference time) for evaluation.\n",
    "\n",
    "We overwrite the `prediction_step` as we need to apply the chat template in order to prompt the model correctly (see also the inference section of this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Union, Tuple\n",
    "from torch import nn\n",
    "from transformers import Seq2SeqTrainer\n",
    "import requests\n",
    "\n",
    "# important: we need to disable caching during training\n",
    "# otherwise the model generates past_key_values which is of type DynamicCache\n",
    "model.config.use_cache = False\n",
    "\n",
    "class Idefics2Trainer(Seq2SeqTrainer):\n",
    "    def prediction_step(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        inputs: Dict[str, Union[torch.Tensor, Any]],\n",
    "        prediction_loss_only: bool,\n",
    "        ignore_keys: Optional[List[str]] = None,\n",
    "        **gen_kwargs,\n",
    "    ) -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Perform an evaluation step on `model` using `inputs`.\n",
    "\n",
    "        Subclass and override to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            model (`nn.Module`):\n",
    "                The model to evaluate.\n",
    "            inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument `labels`. Check your model's documentation for all accepted arguments.\n",
    "            prediction_loss_only (`bool`):\n",
    "                Whether or not to return the loss only.\n",
    "            gen_kwargs:\n",
    "                Additional `generate` specific kwargs.\n",
    "\n",
    "        Return:\n",
    "            Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss, logits and\n",
    "            labels (each being optional).\n",
    "        \"\"\"\n",
    "        if not self.args.predict_with_generate or prediction_loss_only:\n",
    "            return super().prediction_step(\n",
    "                model, inputs, prediction_loss_only=prediction_loss_only, ignore_keys=ignore_keys\n",
    "            )\n",
    "\n",
    "        has_labels = \"labels\" in inputs\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        # Priority (handled in generate):\n",
    "        # non-`None` gen_kwargs > model.generation_config > default GenerationConfig()\n",
    "        if len(gen_kwargs) == 0 and hasattr(self, \"_gen_kwargs\"):\n",
    "            gen_kwargs = self._gen_kwargs.copy()\n",
    "        if \"num_beams\" in gen_kwargs and gen_kwargs[\"num_beams\"] is None:\n",
    "            gen_kwargs.pop(\"num_beams\")\n",
    "        if \"max_length\" in gen_kwargs and gen_kwargs[\"max_length\"] is None:\n",
    "            gen_kwargs.pop(\"max_length\")\n",
    "\n",
    "        default_synced_gpus = False\n",
    "        gen_kwargs[\"synced_gpus\"] = (\n",
    "            gen_kwargs[\"synced_gpus\"] if gen_kwargs.get(\"synced_gpus\") is not None else default_synced_gpus\n",
    "        )\n",
    "\n",
    "        generation_inputs = inputs.copy()\n",
    "        # If the `decoder_input_ids` was created from `labels`, evict the former, so that the model can freely generate\n",
    "        # (otherwise, it would continue generating from the padded `decoder_input_ids`)\n",
    "        if (\n",
    "            \"labels\" in generation_inputs\n",
    "            and \"decoder_input_ids\" in generation_inputs\n",
    "            and generation_inputs[\"labels\"].shape == generation_inputs[\"decoder_input_ids\"].shape\n",
    "        ):\n",
    "            generation_inputs = {\n",
    "                k: v for k, v in inputs.items() if k not in (\"decoder_input_ids\", \"decoder_attention_mask\")\n",
    "            }\n",
    "\n",
    "        # here we need to overwrite the input_ids to only include the prompt\n",
    "        processor = AutoProcessor.from_pretrained(\"HuggingFaceM4/idefics2-8b\", do_image_splitting=False)\n",
    "\n",
    "        # use dummy image\n",
    "        # we can do this since each image is always turned into 64 image tokens\n",
    "        url = \"https://upload.wikimedia.org/wikipedia/commons/f/f3/Zinedine_Zidane_by_Tasnim_03.jpg\"\n",
    "        test_image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "        # prepare prompt for the model\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"Extract JSON.\"},\n",
    "                    {\"type\": \"image\"},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "        prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "        processor_inputs = processor(text=prompt, images=[test_image], return_tensors=\"pt\")\n",
    "        custom_inputs = {}\n",
    "        batch_size = generation_inputs[\"pixel_values\"].shape[0]\n",
    "        device = generation_inputs[\"pixel_values\"].device\n",
    "        custom_inputs[\"input_ids\"] = processor_inputs.input_ids.repeat(batch_size, 1).to(\n",
    "            device\n",
    "        )  # repeat along batch dimension\n",
    "        custom_inputs[\"attention_mask\"] = processor_inputs.attention_mask.repeat(batch_size, 1).to(\n",
    "            device\n",
    "        )  # repeat along batch dimension\n",
    "        custom_inputs[\"pixel_values\"] = generation_inputs[\"pixel_values\"]\n",
    "        custom_inputs[\"pixel_attention_mask\"] = generation_inputs[\"pixel_attention_mask\"]\n",
    "\n",
    "        generated_tokens = self.model.generate(**custom_inputs, **gen_kwargs)\n",
    "\n",
    "        # Strip the prompt from the generated_tokens\n",
    "        generated_tokens = generated_tokens[:, custom_inputs[\"input_ids\"].size(1) :]\n",
    "\n",
    "        # Temporary hack to ensure the generation config is not initialized for each iteration of the evaluation loop\n",
    "        # TODO: remove this hack when the legacy code that initializes generation_config from a model config is\n",
    "        # removed in https://github.com/huggingface/transformers/blob/98d88b23f54e5a23e741833f1e973fdf600cc2c5/src/transformers/generation/utils.py#L1183\n",
    "        if self.model.generation_config._from_model_config:\n",
    "            self.model.generation_config._from_model_config = False\n",
    "\n",
    "        # Retrieves GenerationConfig from model.generation_config\n",
    "        gen_config = self.model.generation_config\n",
    "        # in case the batch is shorter than max length, the output should be padded\n",
    "        if generated_tokens.shape[-1] < gen_config.max_length:\n",
    "            generated_tokens = self._pad_tensors_to_max_len(generated_tokens, gen_config.max_length)\n",
    "        elif gen_config.max_new_tokens is not None and generated_tokens.shape[-1] < gen_config.max_new_tokens + 1:\n",
    "            generated_tokens = self._pad_tensors_to_max_len(generated_tokens, gen_config.max_new_tokens + 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if has_labels:\n",
    "                with self.compute_loss_context_manager():\n",
    "                    outputs = model(**inputs)\n",
    "                if self.label_smoother is not None:\n",
    "                    loss = self.label_smoother(outputs, inputs[\"labels\"]).mean().detach()\n",
    "                else:\n",
    "                    loss = (outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]).mean().detach()\n",
    "            else:\n",
    "                loss = None\n",
    "\n",
    "        if self.args.prediction_loss_only:\n",
    "            return loss, None, None\n",
    "\n",
    "        if has_labels:\n",
    "            labels = inputs[\"labels\"]\n",
    "            if labels.shape[-1] < gen_config.max_length:\n",
    "                labels = self._pad_tensors_to_max_len(labels, gen_config.max_length)\n",
    "            elif gen_config.max_new_tokens is not None and labels.shape[-1] < gen_config.max_new_tokens + 1:\n",
    "                labels = self._pad_tensors_to_max_len(labels, gen_config.max_new_tokens + 1)\n",
    "        else:\n",
    "            labels = None\n",
    "\n",
    "        return loss, generated_tokens, labels\n",
    "\n",
    "    def _pad_tensors_to_max_len(self, tensor, max_length):\n",
    "        pad_token_id = processor.tokenizer.pad_token_id\n",
    "\n",
    "        padded_tensor = pad_token_id * torch.ones(\n",
    "            (tensor.shape[0], max_length), dtype=tensor.dtype, device=tensor.device\n",
    "        )\n",
    "        padded_tensor[:, : tensor.shape[-1]] = tensor\n",
    "        return padded_tensor\n",
    "\n",
    "\n",
    "trainer = Idefics2Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next it's finally time to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Let's see if the model has learned something. We'll take a receipt image of the test set here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example = dataset[\"test\"][0]\n",
    "test_image = test_example[\"image\"]\n",
    "test_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to prepare the image for the model, along with the text prompt we used during training. We need to apply the chat template to make sure the format is respected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare image and prompt for the model\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Extract JSON.\"},\n",
    "            {\"type\": \"image\"},\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we let the model autoregressively generate tokens using the generate() method, which is recommended for use at inference time. This method feeds each predicted token back into the model as conditioning for each next time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(text=prompt, images=[test_image], return_tensors=\"pt\")\n",
    "inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "\n",
    "# Generate\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
    "generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "print(generated_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the Donut model, we could write a `token2json` method which converts the generated token sequence into parsible JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# let's turn that into JSON\n",
    "def token2json(tokens, is_inner_value=False, added_vocab=None):\n",
    "        \"\"\"\n",
    "        Convert a (generated) token sequence into an ordered JSON format.\n",
    "        \"\"\"\n",
    "        if added_vocab is None:\n",
    "            added_vocab = processor.tokenizer.get_added_vocab()\n",
    "\n",
    "        output = {}\n",
    "\n",
    "        while tokens:\n",
    "            start_token = re.search(r\"<s_(.*?)>\", tokens, re.IGNORECASE)\n",
    "            if start_token is None:\n",
    "                break\n",
    "            key = start_token.group(1)\n",
    "            key_escaped = re.escape(key)\n",
    "\n",
    "            end_token = re.search(rf\"</s_{key_escaped}>\", tokens, re.IGNORECASE)\n",
    "            start_token = start_token.group()\n",
    "            if end_token is None:\n",
    "                tokens = tokens.replace(start_token, \"\")\n",
    "            else:\n",
    "                end_token = end_token.group()\n",
    "                start_token_escaped = re.escape(start_token)\n",
    "                end_token_escaped = re.escape(end_token)\n",
    "                content = re.search(\n",
    "                    f\"{start_token_escaped}(.*?){end_token_escaped}\", tokens, re.IGNORECASE | re.DOTALL\n",
    "                )\n",
    "                if content is not None:\n",
    "                    content = content.group(1).strip()\n",
    "                    if r\"<s_\" in content and r\"</s_\" in content:  # non-leaf node\n",
    "                        value = token2json(content, is_inner_value=True, added_vocab=added_vocab)\n",
    "                        if value:\n",
    "                            if len(value) == 1:\n",
    "                                value = value[0]\n",
    "                            output[key] = value\n",
    "                    else:  # leaf nodes\n",
    "                        output[key] = []\n",
    "                        for leaf in content.split(r\"<sep/>\"):\n",
    "                            leaf = leaf.strip()\n",
    "                            if leaf in added_vocab and leaf[0] == \"<\" and leaf[-2:] == \"/>\":\n",
    "                                leaf = leaf[1:-2]  # for categorical special tokens\n",
    "                            output[key].append(leaf)\n",
    "                        if len(output[key]) == 1:\n",
    "                            output[key] = output[key][0]\n",
    "\n",
    "                tokens = tokens[tokens.find(end_token) + len(end_token) :].strip()\n",
    "                if tokens[:6] == r\"<sep/>\":  # non-leaf nodes\n",
    "                    return [output] + token2json(tokens[6:], is_inner_value=True, added_vocab=added_vocab)\n",
    "\n",
    "        if len(output):\n",
    "            return [output] if is_inner_value else output\n",
    "        else:\n",
    "            return [] if is_inner_value else {\"text_sequence\": tokens}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the final JSON!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_json = token2json(generated_texts[0])\n",
    "print(generated_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in generated_json.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
