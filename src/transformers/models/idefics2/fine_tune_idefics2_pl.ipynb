{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune Idefics2 for document parsing (PDF -> JSON)\n",
    "\n",
    "In this notebook, we are going to fine-tune the [Idefics2](https://huggingface.co/docs/transformers/main/en/model_doc/idefics2) model for a document AI use case. Idefics2 is one of the best open-source multimodal models at the time of writing, developed by Hugging Face. Idefics started as a replication of Deepmind's [Flamingo](https://arxiv.org/abs/2204.14198) model, and the second iteration incorporates a lot of advancements in the field such as [NaViT](https://arxiv.org/abs/2307.06304) patching, [Perceiver](https://arxiv.org/abs/2103.03206) resampling and more. It also supports multiple images per training example.\n",
    "\n",
    "However, explaining how Idefics2 works would desire its own video which I might put on YouTube!\n",
    "\n",
    "The goal for the model in this notebook is to generate a JSON that contains key fields (like food items and their corresponding prices) from receipts. We will fine-tune Idefics2 on the [CORD](https://huggingface.co/datasets/naver-clova-ix/cord-v2) dataset, which contains (receipt image, ground truth JSON) pairs.\n",
    "\n",
    "Sources:\n",
    "\n",
    "* Idefics2 [blog post](https://huggingface.co/blog/idefics2)\n",
    "* Idefics2 [DocVQA notebook](https://colab.research.google.com/drive/1NtcTgRbSBKN7pYD3Vdx1j9m8pt3fhFDB?usp=sharing#scrollTo=06CMDrH7Kkdy) on which this notebook is based \n",
    "\n",
    "Note: this notebook is a direct adaptation of my [original Donut notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Donut/CORD/Fine_tune_Donut_on_a_custom_dataset_(CORD)_with_PyTorch_Lightning.ipynb) for Idefics2. You can view Idefics2 as a more powerful Donut model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "Let's start by loading the dataset from the hub. Here we use the [CORD](https://huggingface.co/datasets/naver-clova-ix/cord-v2) dataset, created by the [Donut](https://huggingface.co/docs/transformers/en/model_doc/donut) authors (Donut is another powerful - but slightly undertrained document AI model available in the Transformers library). CORD is an important benchmark for receipt understanding. The Donut authors have prepared it in a format that suits vision-language models: we're going to fine-tune it to generate the JSON given the image.\n",
    "\n",
    "If you want to load your own custom dataset, check out this guide: https://huggingface.co/docs/datasets/image_dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"naver-clova-ix/cord-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As oftentimes, we get a `DatasetDict` which is a dictionary containing 3 splits, one for training, validation and testing. Each split has 2 features, an image and a corresponding ground truth.\n",
    "\n",
    " Let's check the first training example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset['train'][0]\n",
    "image = example[\"image\"]\n",
    "# resize image for smaller displaying\n",
    "width, height = image.size\n",
    "image = image.resize((int(0.3*width), int(0.3*height)))\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the corresponding ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example[\"ground_truth\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! So this contains a ground truth parsing that we want the model to output given an image. We can read it as json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json.loads(example[\"ground_truth\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load processor\n",
    "\n",
    "Next, we'll load the processor which is used to prepare the data in the format that the model expects. Neural networks like Idefics2 don't directly take images and text as input, but rather `pixel_values` (which is a resized, rescaled, normalized and optionally splitted version of the receipt images), `input_ids` (which are text token indices in the vocabulary of the model), etc. This is handled by the processor.\n",
    "\n",
    "### Image splitting\n",
    "\n",
    "Idefics2's processor has a setting called `do_image_splitting` which can be set to `True`/`False`. This defines how images are prepared for the model, either it will just create 1 image (with `do_image_splitting=False`) or it will create multiple (by splitting the images into multiple patches and also including the original image).\n",
    "\n",
    "This has an effect on the amount of memory that's going to be used during training: if we use image splitting we'll encounter more memory usage (as several images are created for each receipt image). We'll use the memory friendly version here. Do note that this has an effect on performance; performance is typically higher with the `do_image_splitting=True` setting. The latter is also how the model which we're going to use (https://huggingface.co/HuggingFaceM4/idefics2-8b) was trained.\n",
    "\n",
    "See also from the [model card](https://huggingface.co/HuggingFaceM4/idefics2-8b):\n",
    "\n",
    "> do_image_splitting=True is especially needed to boost performance on OCR tasks where a very large image is used as input. For the regular VQA or captioning tasks, this argument can be safely set to False with minimal impact on performance (see the evaluation table above).\n",
    "\n",
    "### Image resolution\n",
    "\n",
    "Additionally, one can decrease the maximum image resolution used during training to decrease memory usage. To do so, add `size= {\"longest_edge\": 448, \"shortest_edge\": 378}` when initializing the processor (`AutoProcessor.from_pretrained`). In particular, the longest_edge value can be adapted to fit the need (the default value is 980). We recommend using values that are multiples of 14. There are no changes required on the model side.\n",
    "\n",
    "### Multiple images\n",
    "\n",
    "Note that the processor supports passing multiple images per training example (this is one of the first in the Transformers library)! This is exciting as it means we could fine-tune Idefics2 on PDFs which contain multiple images. It's not the case for this demo notebook (as each receipt only contains a single image), but I could work on a follow-up notebook in which we fine-tune Idefics2 on a multiple-images PDF dataset (as you'll see, the only difference is providing multiple images to the processor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"HuggingFaceM4/idefics2-8b\", do_image_splitting=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model\n",
    "\n",
    "Next, we're going to load the Idefics2 model from the [hub](https://huggingface.co/HuggingFaceM4/idefics2-8b). This is a model with 8 billion trainable parameters. Do note that we load a model here which already has undergone supervised fine-tuning (SFT). The pure pre-trained model (also called \"base model\") is available here: https://huggingface.co/HuggingFaceM4/idefics2-8b-base. We can benefit from the fine-tuning that the model already has undergone.\n",
    "\n",
    "Do note that the Idefics2 team is also going to release a chatty version of Idefics2 optimized for chatbot/AI assistant use cases. However in our case we do not care about the chatty aspect, we just want the model to generate perfect JSON given an image of a receipt.\n",
    "\n",
    "### Full fine-tuning, LoRa and Q-LoRa\n",
    "\n",
    "As this model has 8 billion trainable parameters, that's going to have quite an impact on the amount of memory used. For reference, fine-tuning a model using the [AdamW optimizer](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW) (which is often used to optimize neural networks) with mixed precision, you need about 18 times the amount of parameters in GB of GPU RAM. So in this case, we would need 18x8 billion bytes = 144 GB of GPU RAM if we want to update all the parameters of the model!! That's huge right? And for most people infeasible.\n",
    "\n",
    "Luckily, some clever people came up with the [LoRa](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora) method (LoRa is short for low-rank adapation). It allows to just freeze the existing weights and only train a couple of adapter layers on top of the base model. Hugging Face offers the separate [PEFT library](https://huggingface.co/docs/peft/main/en/index) for easy use of LoRa, along with other Parameter-Efficient Fine-Tuning methods (that's where the name PEFT comes from).\n",
    "\n",
    "Moreover, one can not only freeze the existing base model but also quantize it (which means, shrinking down its size). A neural network's parameters are typically saved in either float32 (which means, 32 bits or 4 bytes are used to store each parameter value) or float16 (which means, 16 bits or half a byte - also called half precision). However, with some clever algorithms one can shrink each parameter to just 8 or 4 bits (half a byte!), without significant effect on final performance. Read all about it here: https://huggingface.co/blog/4bit-transformers-bitsandbytes.\n",
    "\n",
    "This means that we're going to shrink the size of the base Idefics2-8b model considerably using 4-bit quantization, and then only train a couple of adapter layers on top using LoRa (in float16). This idea of combining LoRa with quantization is called Q-LoRa and is the most memory friendly version.\n",
    "\n",
    "Of course, if you have the memory available, feel free to use full fine-tuning or LoRa without quantization! In case of full fine-tuning, the code snippet below instantiates the model with Flash Attention which considerably speeds up computations.\n",
    "\n",
    "There exist many forms of quantization, here we leverage the [BitsAndBytes](https://huggingface.co/docs/transformers/main_classes/quantization#transformers.BitsAndBytesConfig) integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig, Idefics2ForConditionalGeneration\n",
    "from peft import LoraConfig\n",
    "import torch\n",
    "\n",
    "USE_LORA = False\n",
    "USE_QLORA = True\n",
    "\n",
    "# TODO: so far I get OOM when using PeftModel instead of `add_adapter`\n",
    "USE_ADD_ADAPTER = True\n",
    "\n",
    "## Load model\n",
    "\n",
    "# Three options for training, from the lowest precision training to the highest precision training:\n",
    "# - QLora\n",
    "# - Standard Lora\n",
    "# - Full fine-tuning\n",
    "if USE_QLORA or USE_LORA:\n",
    "    if USE_QLORA:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "    model = Idefics2ForConditionalGeneration.from_pretrained(\n",
    "        \"HuggingFaceM4/idefics2-8b\",\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config=bnb_config if USE_QLORA else None,\n",
    "    )\n",
    "    if USE_ADD_ADAPTER:\n",
    "        lora_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=8,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=\".*(text_model|modality_projection|perceiver_resampler).*(down_proj|gate_proj|up_proj|k_proj|q_proj|v_proj|o_proj).*$\",\n",
    "            use_dora=False if USE_QLORA else True,\n",
    "            init_lora_weights=\"gaussian\",\n",
    "        )\n",
    "        model.add_adapter(lora_config)\n",
    "        model.enable_adapters()\n",
    "else:\n",
    "    # for full fine-tuning, we can speed up the model using Flash Attention\n",
    "    # only available on certain devices, see https://github.com/Dao-AILab/flash-attention?tab=readme-ov-file#installation-and-features\n",
    "    model = Idefics2ForConditionalGeneration.from_pretrained(\n",
    "        \"HuggingFaceM4/idefics2-8b\",\n",
    "        torch_dtype=torch.float16,\n",
    "        _attn_implementation=\"flash_attention_2\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_initialized())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PyTorch dataset\n",
    "\n",
    "Next we'll create a regular [PyTorch dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) which defines the individual items of the dataset. For that, one needs to implement 3 methods: an `init` method, a `len` method (which returns the length of the dataset) and a `getitem` method (which returns items of the dataset).\n",
    "\n",
    "The `init` method implements 2 things:\n",
    "* it goes over all the ground truth JSON sequences and turns them into token sequences (which we want the model to generate) using the `json2token` method\n",
    "* it adds special tokens to the model/tokenizer using the `add_tokens` method for which the model will learn an embedding vector. By doing this, keys which occur in the JSON sequences (like `<s_menu>`, `<s_unitprice>` in our case) will get their own token (and corresponding embedding), whereas otherwise these might have been split up into multiple tokens. Do note that I haven't quantified the performance difference regarding this, not sure it helps a lot but the Donut authors did this so I assume it must benefit training.\n",
    "\n",
    "Typically, one uses the processor in the `getitem` method to prepare the data in the format that the model expects, but we'll postpone that here for a reason we'll explain later. In our case we're just going to return 2 things: the image and a corresponding ground truth token sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from typing import Any, List, Dict\n",
    "import random\n",
    "\n",
    "added_tokens = []\n",
    "\n",
    "class Idefics2Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for Idefics2. This class takes a HuggingFace Dataset as input.\n",
    "    \n",
    "    Each row, consists of image path(png/jpg/jpeg) and gt data (json/jsonl/txt).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name_or_path: str,\n",
    "        split: str = \"train\",\n",
    "        sort_json_key: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.split = split\n",
    "        self.sort_json_key = sort_json_key\n",
    "\n",
    "        self.dataset = load_dataset(dataset_name_or_path, split=self.split)\n",
    "        self.dataset_length = len(self.dataset)\n",
    "\n",
    "        self.gt_token_sequences = []\n",
    "        for sample in self.dataset:\n",
    "            ground_truth = json.loads(sample[\"ground_truth\"])\n",
    "            if \"gt_parses\" in ground_truth:  # when multiple ground truths are available, e.g., docvqa\n",
    "                assert isinstance(ground_truth[\"gt_parses\"], list)\n",
    "                gt_jsons = ground_truth[\"gt_parses\"]\n",
    "            else:\n",
    "                assert \"gt_parse\" in ground_truth and isinstance(ground_truth[\"gt_parse\"], dict)\n",
    "                gt_jsons = [ground_truth[\"gt_parse\"]]\n",
    "\n",
    "            self.gt_token_sequences.append(\n",
    "                [\n",
    "                    self.json2token(\n",
    "                        gt_json,\n",
    "                        update_special_tokens_for_json_key=self.split == \"train\",\n",
    "                        sort_json_key=self.sort_json_key,\n",
    "                    )\n",
    "                    for gt_json in gt_jsons  # load json from list of json\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    def json2token(self, obj: Any, update_special_tokens_for_json_key: bool = True, sort_json_key: bool = True):\n",
    "        \"\"\"\n",
    "        Convert an ordered JSON object into a token sequence\n",
    "        \"\"\"\n",
    "        if type(obj) == dict:\n",
    "            if len(obj) == 1 and \"text_sequence\" in obj:\n",
    "                return obj[\"text_sequence\"]\n",
    "            else:\n",
    "                output = \"\"\n",
    "                if sort_json_key:\n",
    "                    keys = sorted(obj.keys(), reverse=True)\n",
    "                else:\n",
    "                    keys = obj.keys()\n",
    "                for k in keys:\n",
    "                    if update_special_tokens_for_json_key:\n",
    "                        self.add_tokens([fr\"<s_{k}>\", fr\"</s_{k}>\"])\n",
    "                    output += (\n",
    "                        fr\"<s_{k}>\"\n",
    "                        + self.json2token(obj[k], update_special_tokens_for_json_key, sort_json_key)\n",
    "                        + fr\"</s_{k}>\"\n",
    "                    )\n",
    "                return output\n",
    "        elif type(obj) == list:\n",
    "            return r\"<sep/>\".join(\n",
    "                [self.json2token(item, update_special_tokens_for_json_key, sort_json_key) for item in obj]\n",
    "            )\n",
    "        else:\n",
    "            obj = str(obj)\n",
    "            if f\"<{obj}/>\" in added_tokens:\n",
    "                obj = f\"<{obj}/>\"  # for categorical special tokens\n",
    "            return obj\n",
    "    \n",
    "    def add_tokens(self, list_of_tokens: List[str]):\n",
    "        \"\"\"\n",
    "        Add special tokens to tokenizer and resize the token embeddings of the decoder\n",
    "        \"\"\"\n",
    "        newly_added_num = processor.tokenizer.add_tokens(list_of_tokens)\n",
    "        if newly_added_num > 0:\n",
    "            model.resize_token_embeddings(len(processor.tokenizer))\n",
    "            added_tokens.extend(list_of_tokens)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Returns one item of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            image : the original Receipt image\n",
    "            target_sequence : tokenized ground truth sequence\n",
    "        \"\"\"\n",
    "        sample = self.dataset[idx]\n",
    "\n",
    "        # inputs\n",
    "        image = sample[\"image\"]\n",
    "        target_sequence = random.choice(self.gt_token_sequences[idx])  # can be more than one, e.g., DocVQA Task 1\n",
    "        \n",
    "        return image, target_sequence    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate the PyTorch datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Idefics2Dataset(\"naver-clova-ix/cord-v2\",  split=\"train\", sort_json_key=False)\n",
    "val_dataset = Idefics2Dataset(\"naver-clova-ix/cord-v2\", split=\"validation\", sort_json_key=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, it's important to check your data. Let's check the first example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_example = train_dataset[0]\n",
    "image, target_sequence = train_example\n",
    "print(target_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply PEFT\n",
    "\n",
    "After resizing the token embedding matrix, we can add our LoRa adapter as an alternative option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "\n",
    "if not USE_ADD_ADAPTER:\n",
    "    lora_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=8,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=\".*(text_model|modality_projection|perceiver_resampler).*(down_proj|gate_proj|up_proj|k_proj|q_proj|v_proj|o_proj).*$\",\n",
    "            use_dora=False if USE_QLORA else True,\n",
    "            init_lora_weights=\"gaussian\",\n",
    "        )\n",
    "\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push resized embedding matrices to the hub\n",
    "\n",
    "Now that we've added special tokens to the model, we'll push them to the hub so that we can use them at inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.get_input_embeddings().weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "\n",
    "# save input embeddings\n",
    "torch.save(model.get_input_embeddings().weight, \"input_embeddings.pt\")\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"input_embeddings.pt\",\n",
    "    path_in_repo=\"input_embeddings.pt\",\n",
    "    repo_id=\"nielsr/idefics2-embeddings\",\n",
    "    repo_type=\"dataset\",\n",
    ")\n",
    "\n",
    "# save output embeddings\n",
    "torch.save(model.get_output_embeddings().weight, \"output_embeddings.pt\")\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"output_embeddings.pt\",\n",
    "    path_in_repo=\"output_embeddings.pt\",\n",
    "    repo_id=\"nielsr/idefics2-embeddings\",\n",
    "    repo_type=\"dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define collate functions\n",
    "\n",
    "Now that we have a PyTorch dataset, we'll define a so-called collator which defines how items of the dataset should be batched together. This is because we typically train neural networks on batches of data (i.e. various images/target sequences combined) rather than one-by-one, using a variant of stochastic-gradient descent or SGD (like Adam, AdamW, etc.).\n",
    "\n",
    "It's only here that we're going to use the processor to turn the (image, target token sequence) into the format that the model expects (which is `pixel_values`, `input_ids` etc.). The reason we do that here is because it allows for **dynamic padding** of the batches: each batch contains images of various resolutions (as Idefics2 preserves the aspect ratio of images) and ground truth sequences of varying lengths. By only using the processor here, we will pad the `pixel_values` up to the largest one in each batch (rather than padding them all to a fixed resolution upfront), and pad the `input_ids` up to the largest sequence in the batch.\n",
    "\n",
    "Important here is that we are calling `apply_chat_template`, which applies a so-called chat template which turns the inputs into the format that the model expects. This is VERY important as this is how the model expects inputs to be formatted!! Read all about it here: https://huggingface.co/docs/transformers/main/en/chat_templating. We'll use the text prompt \"Extract JSON.\" which is also going to be used at inference time. This is just a deliberate choice, we could also not include a text prompt and only train the model on the image inputs.\n",
    "\n",
    "We also decide to limit the length of the text tokens (`input_ids`) to 768 due to memory constraints, feel free to expand if your target token sequences are longer (I'd recommend plotting the average token length of your dataset to determine the optimal value). The Donut authors also used a max length of 768 for CORD.\n",
    "\n",
    "Labels are created for the model by simply copying the inputs to the LLM (`input_ids`), but with padding tokens replaced by the ignore index of the loss function. This ensures that the model doesn't need to learn to predict padding tokens (used to batch examples together). For Idefics2, this is the `image_token_id` as can be seen [here](https://github.com/huggingface/transformers/blob/6f465d45d98f9eaeef83cfdfe79aecc7193b0f1f/src/transformers/models/idefics2/modeling_idefics2.py#L1860).\n",
    "\n",
    "Why are the labels a copy of the model inputs, you may ask? The model will internally shift the labels one position to the right so that the model will learn to predict the next token. This can be seen [here](https://github.com/huggingface/transformers/blob/6f465d45d98f9eaeef83cfdfe79aecc7193b0f1f/src/transformers/models/idefics2/modeling_idefics2.py#L1851-L1855).\n",
    "\n",
    "The collate function for evaluation is different, since there we only need to feed the prompt to the model, as we'll use the `generate()` method to autoregressively generate a completion. Hence we only need to prepare the prompt for the model (notice the `add_generation_prompt=True` flag and the fact that we're not including an \"assistant\" response). We also pass the ground truth token sequences (`answers`) as this allows us to compute evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_token_id = processor.tokenizer.additional_special_tokens_ids[processor.tokenizer.additional_special_tokens.index(\"<image>\")]\n",
    "MAX_LENGTH = 768\n",
    "\n",
    "def train_collate_fn(examples):\n",
    "    texts = []\n",
    "    images = []\n",
    "    for example in examples:\n",
    "        image, ground_truth = example\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"Extract JSON.\"},\n",
    "                    {\"type\": \"image\"},\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": ground_truth}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        text = processor.apply_chat_template(messages, add_generation_prompt=False)\n",
    "        texts.append(text.strip())\n",
    "        images.append([image])\n",
    "\n",
    "    batch = processor(text=texts, images=images, padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = image_token_id\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    pixel_values = batch[\"pixel_values\"]\n",
    "    pixel_attention_mask = batch[\"pixel_attention_mask\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    \n",
    "    return input_ids, attention_mask, pixel_values, pixel_attention_mask, labels\n",
    "\n",
    "\n",
    "def eval_collate_fn(examples):\n",
    "    \n",
    "    # we feed the prompt to the model\n",
    "    images = []\n",
    "    texts = []\n",
    "    answers = []\n",
    "    for example in examples:\n",
    "        image, ground_truth = example\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"Extract JSON.\"},\n",
    "                    {\"type\": \"image\"},\n",
    "                ]\n",
    "            },\n",
    "        ]\n",
    "        text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "        images.append([image])\n",
    "        texts.append(text.strip())\n",
    "        answers.append(ground_truth)\n",
    "\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True) \n",
    "\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    pixel_values = batch[\"pixel_values\"]\n",
    "    pixel_attention_mask = batch[\"pixel_attention_mask\"]\n",
    "    \n",
    "    return input_ids, attention_mask, pixel_values, pixel_attention_mask, answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define PyTorch LightningModule\n",
    "\n",
    "There are various ways to train a PyTorch model: one could just use native PyTorch, use the [Trainer API](https://huggingface.co/docs/transformers/en/main_classes/trainer) or frameworks like [Accelerate](https://huggingface.co/docs/accelerate/en/index). In this notebook, I'll use PyTorch Lightning as it allows to easily compute evaluation metrics during training.\n",
    "\n",
    "Below, we define a [LightningModule](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html), which is the standard way to train a model in PyTorch Lightning. A LightningModule is an `nn.Module` with some additional functionality.\n",
    "\n",
    "Basically, PyTorch Lightning will take care of all device placements (`.to(device)`) for us, as well as the backward pass, putting the model in training mode, etc.\n",
    "\n",
    "Notice the difference between a training step and an evaluation step:\n",
    "\n",
    "- a training step only consists of a forward pass, in which we compute the cross-entropy loss between the model's next token predictions and the ground truth (in parallel for all tokens, this technique is known as \"teacher forcing\"). The backward pass is handled by PyTorch Lightning.\n",
    "- an evaluation step consists of making the model autoregressively complete the prompt using the [`generate()`](https://huggingface.co/docs/transformers/v4.40.1/en/main_classes/text_generation#transformers.GenerationMixin.generate) method. After that, we compute an evaluation metric between the predicted sequences and the ground truth ones. This allows us to see how the model is improving over the course of training. The metric we use here is the so-called [Levenhstein edit distance](https://en.wikipedia.org/wiki/Levenshtein_distance). This quantifies how much we would need to edit the predicted token sequence to get the target sequence (the fewer edits the better!). Its optimal value is 0 (which means, no edits need to be made).\n",
    "\n",
    "Besides that, we define the optimizer to use ([AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) is a good default choice) and the data loaders, which use the collate functions defined above to batch together items of the PyTorch datasets. Do note that AdamW is a pretty heavy optimizer in terms of memory requirements, but as we're training with QLoRa we only need to store optimizer states for the adapter layers. For full fine-tuning, one could take a look at more memory friendly optimizers such as [8-bit Adam](https://huggingface.co/docs/bitsandbytes/main/en/optimizers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from torch.utils.data import DataLoader\n",
    "import re\n",
    "from nltk import edit_distance\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Idefics2ModelPLModule(L.LightningModule):\n",
    "    def __init__(self, config, processor, model):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "\n",
    "        self.batch_size = config.get(\"batch_size\")\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        input_ids, attention_mask, pixel_values, pixel_attention_mask, labels = batch\n",
    "\n",
    "        outputs = self.model(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                pixel_values=pixel_values,\n",
    "                                pixel_attention_mask=pixel_attention_mask,\n",
    "                                labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataset_idx=0):\n",
    "        \n",
    "        input_ids, attention_mask, pixel_values, pixel_attention_mask, answers = batch\n",
    "\n",
    "        # autoregressively generate token IDs\n",
    "        generated_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                                       pixel_values=pixel_values, pixel_attention_mask=pixel_attention_mask,\n",
    "                                       max_new_tokens=768)\n",
    "        # turn them back into text, chopping of the prompt\n",
    "        # important: we don't skip special tokens here, because we want to see them in the output\n",
    "        predictions = self.processor.batch_decode(generated_ids[:, input_ids.size(1):], skip_special_tokens=True)\n",
    "\n",
    "        scores = []\n",
    "        for pred, answer in zip(predictions, answers):\n",
    "            pred = re.sub(r\"(?:(?<=>) | (?=</s_))\", \"\", pred)\n",
    "            scores.append(edit_distance(pred, answer) / max(len(pred), len(answer)))\n",
    "\n",
    "            if self.config.get(\"verbose\", False) and len(scores) == 1:\n",
    "                print(f\"Prediction: {pred}\")\n",
    "                print(f\"    Answer: {answer}\")\n",
    "                print(f\" Normed ED: {scores[0]}\")\n",
    "\n",
    "        self.log(\"val_edit_distance\", np.mean(scores))\n",
    "        \n",
    "        return scores\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # you could also add a learning rate scheduler if you want\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.config.get(\"lr\"))\n",
    "    \n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(train_dataset, collate_fn=train_collate_fn, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(val_dataset, collate_fn=eval_collate_fn, batch_size=self.batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate it (based on a config dictionary which defines all hyperparameters for training).\n",
    "\n",
    "The batch size was determined based on PyTorch Lightning's [auto batch size finder](https://lightning.ai/docs/pytorch/stable/advanced/training_tricks.html#batch-size-finder) feature. It tries to find the biggest batch size for your given hardware.\n",
    "\n",
    "Do note that one can play around with the hyperparameters, I just use good defaults here: 10 epochs, a learning rate of 1e-4 which I found in the original Idefics2 notebook (linked at the top of this notebook), use mixed precision for training (more memory friendly). One could extend this with things like gradient accumulation and gradient checkpointing.\n",
    "\n",
    "I recommend [this guide](https://huggingface.co/docs/transformers/v4.20.1/en/perf_train_gpu_one) which goes over all tips and tricks regarding maximizing fine-tuning performance on consumer hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"max_epochs\": 10,\n",
    "          # \"val_check_interval\": 0.2, # how many times we want to validate during an epoch\n",
    "          \"check_val_every_n_epoch\": 1,\n",
    "          \"gradient_clip_val\": 1.0,\n",
    "          \"accumulate_grad_batches\": 8,\n",
    "          \"lr\": 1e-4,\n",
    "          \"batch_size\": 2,\n",
    "          \"precision\": \"16-mixed\", # we'll use mixed precision\n",
    "          # \"seed\":2022, # can be used for reproducibility\n",
    "          \"warmup_steps\": 50,\n",
    "          \"result_path\": \"./result\",\n",
    "          \"verbose\": True,\n",
    "}\n",
    "\n",
    "model_module = Idefics2ModelPLModule(config, processor, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define callbacks\n",
    "\n",
    "Optionally, Lightning allows to define so-called [callbacks](https://lightning.ai/docs/pytorch/stable/extensions/callbacks.html), which are arbitrary pieces of code that can be executed during training.\n",
    "\n",
    "Here I'm adding a `PushToHubCallback` which will push the model to the [hub](https://huggingface.co/) at the end of every epoch as well as at the end of training. Do note that you could of course also pass the `private=True` flag when pushing to the hub, if you wish to keep your model private. Hugging Face also offers the [Enterprise Hub](https://huggingface.co/enterprise) so that you can easily share models with your colleagues privately in a secure way.\n",
    "\n",
    "We'll also use the EarlyStopping callback of Lightning, which will automatically stop training once the evaluation metric (edit distance in our case) doesn't improve after 3 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import Callback\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "MODEL_REPO_ID = \"nielsr/idefics2-cord-demo-v2\"\n",
    "\n",
    "class PushToHubCallback(Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        print(f\"Pushing model to the hub, epoch {trainer.current_epoch}\")\n",
    "        pl_module.model.push_to_hub(MODEL_REPO_ID,\n",
    "                                    commit_message=f\"Training in progress, epoch {trainer.current_epoch}\")\n",
    "\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        print(f\"Pushing model to the hub after training\")\n",
    "        pl_module.processor.push_to_hub(MODEL_REPO_ID,\n",
    "                                    commit_message=f\"Training done\")\n",
    "        pl_module.model.push_to_hub(MODEL_REPO_ID,\n",
    "                                    commit_message=f\"Training done\")\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_edit_distance\", patience=3, verbose=False, mode=\"min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train!\n",
    "\n",
    "Alright, we're set to start training! We will also pass the Weights and Biases logger so that we get see some pretty plots of our loss and evaluation metric during training (do note that you may need to log in the first time you run this, see the [docs](https://docs.wandb.ai/guides/integrations/lightning)).\n",
    "\n",
    "Do note that this Trainer class supports many more flags! See the docs: https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.trainer.trainer.Trainer.html#lightning.pytorch.trainer.trainer.Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "wandb_logger = WandbLogger(project=\"Idefics2-PL\", name=\"demo-run-cord\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "        accelerator=\"gpu\",\n",
    "        devices=[0],\n",
    "        max_epochs=config.get(\"max_epochs\"),\n",
    "        check_val_every_n_epoch=config.get(\"check_val_every_n_epoch\"),\n",
    "        gradient_clip_val=config.get(\"gradient_clip_val\"),\n",
    "        accumulate_grad_batches=config.get(\"accumulate_grad_batches\"),\n",
    "        precision=config.get(\"precision\"),\n",
    "        num_sanity_val_steps=0,\n",
    "        logger=wandb_logger,\n",
    "        callbacks=[PushToHubCallback(), early_stop_callback],\n",
    ")\n",
    "\n",
    "trainer.fit(model_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Let's see if the model has learned something. We'll load the model from the hub first. Notice that, as we only trained adapters on top of the base model, the [repository on the hub](https://huggingface.co/nielsr/idefics2-cord-demo) to which we pushed only contains the weights and configuration of the adapters. This is a very lightweight file smaller than 100 MB.\n",
    "\n",
    "Thanks to the PEFT integration in Transformers, the `from_pretrained` method will automatically load the weights of the base model ([Idefics2-8B](https://huggingface.co/HuggingFaceM4/idefics2-8b) in our case) as well as the adapter weights.\n",
    "\n",
    "To reduce inference costs, we'll again load the model in 4 bits by passing a `quantization_config`, in order to reduce memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, BitsAndBytesConfig, Idefics2ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "peft_model_id = \"nielsr/idefics2-cord-demo\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(peft_model_id)\n",
    "\n",
    "# Define quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "# Load the base model with adapters on top\n",
    "model = Idefics2ForConditionalGeneration.from_pretrained(\n",
    "    peft_model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=quantization_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to load the resized token embedding matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "filepath = hf_hub_download(repo_id=\"nielsr/idefics2-embeddings\", filename=\"input_embeddings.pt\", repo_type=\"dataset\")\n",
    "filepath = hf_hub_download(repo_id=\"nielsr/idefics2-embeddings\", filename=\"output_embeddings.pt\", repo_type=\"dataset\")\n",
    "\n",
    "input_embeddings = torch.load(filepath / \"input_embeddings.pt\")\n",
    "output_embeddings = torch.load(filepath / \"output_embeddings.pt\")\n",
    "\n",
    "model.resize_token_embeddings(len(processor.tokenizer))\n",
    "model.set_input_embeddings(input_embeddings)\n",
    "model.set_output_embeddings(output_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll take a receipt image of the test set here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example = dataset[\"test\"][0]\n",
    "test_image = test_example[\"image\"]\n",
    "test_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to prepare the image for the model, along with the text prompt we used during training. We need to apply the chat template to make sure the format is respected.\n",
    "\n",
    "Notice that this is exactly the same as what we did for the evaluation data collate function. We need to pass `add_generation_prompt=True` in order to add the prompt with \"Assistant:\" so that the model will continue it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare image and prompt for the model\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Extract JSON.\"},\n",
    "            {\"type\": \"image\"},\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we let the model autoregressively generate tokens using the [generate()](https://huggingface.co/docs/transformers/v4.40.1/en/main_classes/text_generation#transformers.GenerationMixin.generate) method, which is recommended for use at inference time. This method feeds each predicted token back into the model as conditioning for each next time step.\n",
    "\n",
    "Do note that there are various ways of decoding text, here we use greedy decoding which is the default. There are various fancier methods such as beam search and top-k sampling. Refer to [this amazing blog post](https://huggingface.co/blog/how-to-generate) for all details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "inputs = processor(text=prompt, images=[test_image], return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "# Generate token IDs\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=768)\n",
    "\n",
    "# Decode back into text\n",
    "generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the Donut model, we could write a `token2json` method which converts the generated token sequence into parsible JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# let's turn that into JSON\n",
    "def token2json(tokens, is_inner_value=False, added_vocab=None):\n",
    "        \"\"\"\n",
    "        Convert a (generated) token sequence into an ordered JSON format.\n",
    "        \"\"\"\n",
    "        if added_vocab is None:\n",
    "            added_vocab = processor.tokenizer.get_added_vocab()\n",
    "\n",
    "        output = {}\n",
    "\n",
    "        while tokens:\n",
    "            start_token = re.search(r\"<s_(.*?)>\", tokens, re.IGNORECASE)\n",
    "            if start_token is None:\n",
    "                break\n",
    "            key = start_token.group(1)\n",
    "            key_escaped = re.escape(key)\n",
    "\n",
    "            end_token = re.search(rf\"</s_{key_escaped}>\", tokens, re.IGNORECASE)\n",
    "            start_token = start_token.group()\n",
    "            if end_token is None:\n",
    "                tokens = tokens.replace(start_token, \"\")\n",
    "            else:\n",
    "                end_token = end_token.group()\n",
    "                start_token_escaped = re.escape(start_token)\n",
    "                end_token_escaped = re.escape(end_token)\n",
    "                content = re.search(\n",
    "                    f\"{start_token_escaped}(.*?){end_token_escaped}\", tokens, re.IGNORECASE | re.DOTALL\n",
    "                )\n",
    "                if content is not None:\n",
    "                    content = content.group(1).strip()\n",
    "                    if r\"<s_\" in content and r\"</s_\" in content:  # non-leaf node\n",
    "                        value = token2json(content, is_inner_value=True, added_vocab=added_vocab)\n",
    "                        if value:\n",
    "                            if len(value) == 1:\n",
    "                                value = value[0]\n",
    "                            output[key] = value\n",
    "                    else:  # leaf nodes\n",
    "                        output[key] = []\n",
    "                        for leaf in content.split(r\"<sep/>\"):\n",
    "                            leaf = leaf.strip()\n",
    "                            if leaf in added_vocab and leaf[0] == \"<\" and leaf[-2:] == \"/>\":\n",
    "                                leaf = leaf[1:-2]  # for categorical special tokens\n",
    "                            output[key].append(leaf)\n",
    "                        if len(output[key]) == 1:\n",
    "                            output[key] = output[key][0]\n",
    "\n",
    "                tokens = tokens[tokens.find(end_token) + len(end_token) :].strip()\n",
    "                if tokens[:6] == r\"<sep/>\":  # non-leaf nodes\n",
    "                    return [output] + token2json(tokens[6:], is_inner_value=True, added_vocab=added_vocab)\n",
    "\n",
    "        if len(output):\n",
    "            return [output] if is_inner_value else output\n",
    "        else:\n",
    "            return [] if is_inner_value else {\"text_sequence\": tokens}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the final JSON!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_json = token2json(generated_texts[0])\n",
    "print(generated_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in generated_json.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
